import numpy as np
import random

# Environment settings
GRID_SIZE = 10
NUM_DRONES = 3
NUM_TARGETS = 3
EPISODES = 200
ACTIONS = 5  # up, down, left, right, stay

# Q-table: agent x state x action
Q = [{} for _ in range(NUM_DRONES)]

alpha = 0.1
gamma = 0.9
epsilon = 0.2

def get_state(drone_pos, targets):
    nearest_target = min(targets, key=lambda t: abs(t[0]-drone_pos[0]) + abs(t[1]-drone_pos[1]))
    return (drone_pos[0], drone_pos[1],
            nearest_target[0]-drone_pos[0],
            nearest_target[1]-drone_pos[1])

def move(pos, action):
    x, y = pos
    if action == 0 and y < GRID_SIZE-1: y += 1
    elif action == 1 and y > 0: y -= 1
    elif action == 2 and x > 0: x -= 1
    elif action == 3 and x < GRID_SIZE-1: x += 1
    return (x, y)

results = []

for episode in range(EPISODES):
    drones = [(random.randint(0, GRID_SIZE-1), random.randint(0, GRID_SIZE-1)) for _ in range(NUM_DRONES)]
    targets = [(random.randint(0, GRID_SIZE-1), random.randint(0, GRID_SIZE-1)) for _ in range(NUM_TARGETS)]
    
    total_reward = 0
    collisions = 0

    for step in range(50):
        actions = []
        states = []

        for i in range(NUM_DRONES):
            state = get_state(drones[i], targets)
            states.append(state)

            if state not in Q[i]:
                Q[i][state] = np.zeros(ACTIONS)

            if random.random() < epsilon:
                action = random.randint(0, ACTIONS-1)
            else:
                action = np.argmax(Q[i][state])

            actions.append(action)

        new_positions = [move(drones[i], actions[i]) for i in range(NUM_DRONES)]

        # Collision detection
        if len(set(new_positions)) < NUM_DRONES:
            reward = -5
            collisions += 1
        else:
            reward = -0.1

        # Target detection
        for pos in new_positions:
            if pos in targets:
                reward += 10
                targets.remove(pos)

        total_reward += reward

        # Q-learning update (centralized reward)
        for i in range(NUM_DRONES):
            next_state = get_state(new_positions[i], targets) if targets else states[i]
            if next_state not in Q[i]:
                Q[i][next_state] = np.zeros(ACTIONS)

            Q[i][states[i]][actions[i]] += alpha * (
                reward + gamma * np.max(Q[i][next_state]) - Q[i][states[i]][actions[i]]
            )

        drones = new_positions
        if not targets:
            break

    results.append(total_reward)

print("Training completed")
print("Average reward:", sum(results)/len(results))
print("Final episode reward:", results[-1])
