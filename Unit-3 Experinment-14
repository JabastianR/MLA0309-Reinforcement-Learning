import numpy as np
import random
from collections import deque

# ============================
# RTS ENVIRONMENT
# ============================
class RTSEnvironment:
    def reset(self):
        self.resources = 50
        self.units = 1
        self.enemy_health = 100
        return self.get_state()

    def get_state(self):
        return np.array([self.resources, self.units, self.enemy_health], dtype=float)

    def step(self, action):
        gather, build, attack = action

        # Gather resources
        self.resources += max(0, int(gather * 10))

        # Build units
        if self.resources >= 10 and build > 0.5:
            self.units += 1
            self.resources -= 10

        # Attack
        damage = int(max(0, attack) * self.units * 5)
        self.enemy_health -= damage

        reward = damage + gather * 2
        done = self.enemy_health <= 0

        return self.get_state(), reward, done


# ============================
# REPLAY BUFFER
# ============================
class ReplayBuffer:
    def __init__(self, size=5000):
        self.buffer = deque(maxlen=size)

    def add(self, transition):
        self.buffer.append(transition)

    def sample(self, batch_size):
        return random.sample(self.buffer, batch_size)

    def __len__(self):
        return len(self.buffer)


# ============================
# SIMPLE NEURAL NETWORK (NUMPY)
# ============================
class SimpleNetwork:
    def __init__(self, input_dim, output_dim):
        self.W1 = np.random.randn(input_dim, 16) * 0.1
        self.W2 = np.random.randn(16, output_dim) * 0.1

    def forward(self, x):
        h = np.tanh(np.dot(x, self.W1))
        out = np.tanh(np.dot(h, self.W2))
        return out


# ============================
# DDPG-LIKE AGENT
# ============================
class DDPGAgent:
    def __init__(self, state_dim, action_dim):
        self.actor = SimpleNetwork(state_dim, action_dim)
        self.buffer = ReplayBuffer()
        self.gamma = 0.95
        self.lr = 0.001

    def select_action(self, state):
        return self.actor.forward(state)

    def train(self, batch_size=32):
        if len(self.buffer) < batch_size:
            return

        batch = self.buffer.sample(batch_size)
        for state, action, reward, next_state, done in batch:
            predicted_action = self.actor.forward(state)
            error = reward - np.sum(predicted_action)
            self.actor.W2 += self.lr * error
            self.actor.W1 += self.lr * error


# ============================
# TRAINING LOOP
# ============================
env = RTSEnvironment()
agent = DDPGAgent(state_dim=3, action_dim=3)

episodes = 200

for episode in range(episodes):
    state = env.reset()
    total_reward = 0

    for step in range(200):
        action = agent.select_action(state)
        next_state, reward, done = env.step(action)

        agent.buffer.add((state, action, reward, next_state, done))
        agent.train()

        state = next_state
        total_reward += reward

        if done:
            break

    print(f"Episode {episode + 1}, Total Reward: {total_reward:.2f}")
